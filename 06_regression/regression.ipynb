{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Homework 3: Linear Regression, Logistic Regression, Model Selection\n\n**EEE 598**\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nimport pandas as pd\nimport os\n\nos.makedirs('results', exist_ok=True)\nnp.random.seed(42)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import tensorflow as tf\n\nmnist = tf.keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\nidx_train = np.where((train_labels == 2) | (train_labels == 6))[0]\nidx_test  = np.where((test_labels  == 2) | (test_labels  == 6))[0]\n\nX_all = train_images[idx_train].reshape(len(idx_train), -1) / 255.0\ny_all = np.where(train_labels[idx_train] == 6, 1, -1)\n\nX_test = test_images[idx_test].reshape(len(idx_test), -1) / 255.0\ny_test = np.where(test_labels[idx_test] == 6, 1, -1)\n\nX_train_1000 = X_all[:1000];  y_train_1000 = y_all[:1000]\nX_train_300  = X_all[:300];   y_train_300  = y_all[:300]\n\nprint(f\"X_train_1000: {X_train_1000.shape}  y_train_1000: {y_train_1000.shape}\")\nprint(f\"X_train_300 : {X_train_300.shape}   y_train_300 : {y_train_300.shape}\")\nprint(f\"X_test      : {X_test.shape}        y_test      : {y_test.shape}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Section 1: Model Selection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rbf_kernel_matrix(X1, X2, sigma):\n    sq1 = np.sum(X1**2, axis=1, keepdims=True)\n    sq2 = np.sum(X2**2, axis=1, keepdims=True)\n    dist_sq = sq1 + sq2.T - 2.0 * (X1 @ X2.T)\n    dist_sq = np.maximum(dist_sq, 0.0)\n    return np.exp(-dist_sq / (2.0 * sigma**2))\n\ndef fit_kernel_svm(X, y, sigma, maxiter=300):\n    n = len(y)\n    K = rbf_kernel_matrix(X, X, sigma)\n    G = K * np.outer(y, y)\n\n    def obj(a):   return -np.sum(a) + 0.5 * a @ G @ a\n    def jac(a):   return -np.ones(n) + G @ a\n\n    constraints = (\n        {'type': 'ineq', 'fun': lambda a: a,     'jac': lambda a: np.eye(n)},\n        {'type': 'eq',   'fun': lambda a: a @ y, 'jac': lambda a: y}\n    )\n    res = minimize(obj, np.zeros(n), jac=jac, method='SLSQP',\n                   constraints=constraints,\n                   options={'maxiter': maxiter, 'ftol': 1e-6, 'disp': False})\n    alpha = np.maximum(res.x, 0.0)\n    sv = alpha > 1e-5\n    b = np.mean(y[sv] - (K[:, sv][sv].T @ (alpha[sv] * y[sv]))) if sv.any() else 0.0\n    return alpha, b, sv\n\ndef predict_kernel_svm(X_tr, y_tr, alpha, b, sigma, X_te):\n    K = rbf_kernel_matrix(X_te, X_tr, sigma)\n    scores = K @ (alpha * y_tr) + b\n    preds  = np.sign(scores)\n    preds[preds == 0] = 1\n    return preds\n\ndef zero_one_loss(y_true, y_pred):\n    return np.mean(y_true != y_pred)\n\ndef train_perceptron(X, y, alpha=0.15, max_epochs=100):\n    w = np.zeros(X.shape[1])\n    for epoch in range(max_epochs):\n        updates = 0\n        for i in range(len(X)):\n            if np.sign(X[i] @ w) != y[i]:\n                w += alpha * y[i] * X[i]\n                updates += 1\n        if updates == 0:\n            break\n    return w\n\ndef knn_predict(X_tr, y_tr, X_te, k):\n    sq_tr = np.sum(X_tr**2, axis=1)\n    sq_te = np.sum(X_te**2, axis=1, keepdims=True)\n    dist_sq = sq_te + sq_tr - 2.0 * (X_te @ X_tr.T)\n    idx = np.argpartition(dist_sq, k, axis=1)[:, :k]\n    votes = y_tr[idx].sum(axis=1)\n    return np.where(votes >= 0, 1, -1)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 1(a) \u2014 k-SVM \u03c3 Sweep"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sigma_values = [0.2, 0.5, 1, 3, 4, 5, 10]\nksvm_results = []\n\nfor sigma in sigma_values:\n    alpha, b, sv = fit_kernel_svm(X_train_1000, y_train_1000, sigma)\n    train_pred = predict_kernel_svm(X_train_1000, y_train_1000, alpha, b, sigma, X_train_1000)\n    test_pred  = predict_kernel_svm(X_train_1000, y_train_1000, alpha, b, sigma, X_test)\n    tr_loss = zero_one_loss(y_train_1000, train_pred)\n    te_loss = zero_one_loss(y_test,       test_pred)\n    ksvm_results.append({'model': f'k-SVM \u03c3={sigma}',\n                         'train_loss': tr_loss, 'test_loss': te_loss,\n                         'n_sv': sv.sum()})\n    print(f\"\u03c3={sigma:<4}  train={tr_loss:.4f}  test={te_loss:.4f}  SVs={sv.sum()}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 1(b) \u2014 Discussion\n\nFrom the results in 1(a), I observed that \u03c3 controls how quickly the RBF kernel similarity falls off with distance. When I used a very small \u03c3 like 0.2, the training error dropped to zero because the kernel only pays attention to very nearby points, essentially memorising the training set \u2014 this is overfitting. On the other hand, with a large \u03c3 like 10, the kernel similarity becomes nearly constant everywhere and the decision boundary collapses toward a linear one, causing higher error on both training and test sets \u2014 this is underfitting.\n\nThe intermediate values of \u03c3 gave the best test performance by balancing between these extremes. To properly tune \u03c3, I should not use the test set, since selecting based on test performance gives an overly optimistic estimate of generalisation error. Instead, I used 10-fold cross-validation in Section 4, which estimates the generalisation error using only training data and picks the \u03c3 with the lowest average validation error.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 1(c) \u2014 Perceptron, k-NN, Hard-Margin SVM on 300 Samples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "X_bias_300  = np.c_[np.ones(len(X_train_300)),  X_train_300]\nX_bias_test = np.c_[np.ones(len(X_test)),        X_test]\n\nw_p = train_perceptron(X_bias_300, y_train_300)\ntr_p = zero_one_loss(y_train_300, np.sign(X_bias_300  @ w_p))\nte_p = zero_one_loss(y_test,       np.sign(X_bias_test @ w_p))\nprint(f\"Perceptron   train={tr_p:.4f}  test={te_p:.4f}\")\n\nknn_rows = []\nfor k in [3, 5]:\n    tr_k = zero_one_loss(y_train_300, knn_predict(X_train_300, y_train_300, X_train_300, k))\n    te_k = zero_one_loss(y_test,       knn_predict(X_train_300, y_train_300, X_test, k))\n    knn_rows.append({'model': f'{k}-nearest neighbor', 'train_loss': tr_k, 'test_loss': te_k})\n    print(f\"{k}-NN         train={tr_k:.4f}  test={te_k:.4f}\")\n\nn_feat = X_train_300.shape[1]\nsvm_res = minimize(\n    lambda w: 0.5 * np.linalg.norm(w[:n_feat])**2,\n    np.zeros(n_feat + 1), method='SLSQP',\n    constraints={'type': 'ineq',\n                 'fun': lambda w: y_train_300 * (X_train_300 @ w[:n_feat] + w[n_feat]) - 1},\n    options={'maxiter': 500, 'ftol': 1e-8, 'disp': False}\n)\nw_s, b_s = svm_res.x[:n_feat], svm_res.x[n_feat]\ntr_s = zero_one_loss(y_train_300, np.sign(X_train_300 @ w_s + b_s))\nte_s = zero_one_loss(y_test,       np.sign(X_test      @ w_s + b_s))\nprint(f\"SVM          train={tr_s:.4f}  test={te_s:.4f}  ||w||={np.linalg.norm(w_s):.4f}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "all_rows = (\n    ksvm_results\n    + [{'model': 'Perceptron', 'train_loss': tr_p, 'test_loss': te_p}]\n    + [{'model': 'SVM',        'train_loss': tr_s, 'test_loss': te_s}]\n    + knn_rows\n)\ndf = pd.DataFrame(all_rows)[['model', 'train_loss', 'test_loss']]\nprint(\"Model Selection Summary\")\nprint(df.to_string(index=False))\n\nfig, ax = plt.subplots(figsize=(13, 5))\nx  = np.arange(len(df))\nw  = 0.38\nb1 = ax.bar(x - w/2, df['train_loss'], w, label='Train 0-1 Loss', color='steelblue', alpha=0.85)\nb2 = ax.bar(x + w/2, df['test_loss'],  w, label='Test 0-1 Loss',  color='tomato',    alpha=0.85)\nax.set_xticks(x); ax.set_xticklabels(df['model'], rotation=35, ha='right', fontsize=9)\nax.set_ylabel('0-1 Loss'); ax.set_title('Model Selection: Train vs Test Loss (MNIST 2 vs 6)')\nax.legend(); ax.grid(True, alpha=0.3, axis='y')\nfor bar in list(b1)+list(b2):\n    h = bar.get_height()\n    if h > 0:\n        ax.text(bar.get_x()+bar.get_width()/2, h+0.002, f'{h:.3f}',\n                ha='center', va='bottom', fontsize=7)\nplt.tight_layout()\nplt.savefig('results/model_selection_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 1(d) \u2014 Discussion\n\nTo choose between different algorithms, I compared their test errors after training on the same 300-sample subset. From my results, the models with the lowest test error were the best generalisers. I also observed that models with very low training error but high test error (like k-SVM with small \u03c3) were overfitting, while models with both high training and test error were underfitting.\n\nThe right way to do model selection is to use cross-validation on the training data only, compare the cross-validated errors, and pick the model that minimises that. The test set should only be used at the very end to report final performance \u2014 using it during selection gives a misleadingly optimistic estimate because the model is effectively being tuned on the test data.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Section 2: Linear Regression"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 2.1 \u2014 Manual Gradient Descent Trace\n\nI ran 5 iterations of gradient descent on the two training samples with $\\theta=[0,0,0]^\\top$ and $\\alpha=0.1$:\n\n$$\\theta \\leftarrow \\theta - \\alpha \\nabla L(\\theta), \\qquad \\nabla L(\\theta) = \\sum_{i=1}^{2}(\\hat{y}^{(i)} - y^{(i)})\\, x^{(i)}$$\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "X_lr = np.array([[1., 2., 1.],\n                 [2., 3., 1.]])\ny_lr = np.array([3., 4.])\ntheta = np.zeros(3)\nalpha_lr = 0.1\n\nprint(f\"{'Iter':>4}  {'theta':>35}  {'grad':>35}  {'Loss':>8}\")\nprint(\"-\"*90)\nfor it in range(6):\n    preds  = X_lr @ theta\n    errors = preds - y_lr\n    grad   = X_lr.T @ errors\n    loss   = 0.5 * np.sum(errors**2)\n    print(f\"{it:>4}  {str(np.round(theta,4)):>35}  {str(np.round(grad,4)):>35}  {loss:>8.4f}\")\n    if it < 5:\n        theta = theta - alpha_lr * grad\n\nprint(f\"\\nFinal theta = {theta}\")\nprint(f\"Predictions: y1_hat={X_lr[0]@theta:.4f}, y2_hat={X_lr[1]@theta:.4f}  (true: 3, 4)\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 2.3 \u2014 Polynomial Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "np.random.seed(42)\nnoise  = np.random.normal(0, 1, 10)\nx_data = (np.arange(10) - 5) / 2\ny_data = x_data**3 - 2*x_data**2 + 1 + noise\n\nx_smooth = np.linspace(x_data.min(), x_data.max(), 300)\ny_true   = x_smooth**3 - 2*x_smooth**2 + 1\n\ndegrees = [1, 2, 3, 4, 5]\ncolors  = ['royalblue', 'darkorange', 'green', 'red', 'purple']\nmodels_poly = {}\n\nprint(f\"{'Degree':>7}  {'Train MSE':>12}  {'Coefficients'}\")\nprint(\"-\"*70)\nfor deg in degrees:\n    theta_p = np.polyfit(x_data, y_data, deg)\n    y_fit   = np.polyval(theta_p, x_data)\n    mse     = np.mean((y_data - y_fit)**2)\n    models_poly[deg] = theta_p\n    print(f\"{deg:>7}  {mse:>12.4f}  {np.round(theta_p, 3)}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\nax = axes[0]\nax.scatter(x_data, y_data, color='black', s=70, zorder=5, label='Training data')\nax.plot(x_smooth, y_true, 'k--', lw=2.5, label='True: $y=x^3-2x^2+1$', zorder=4)\nfor deg, color in zip(degrees, colors):\n    ax.plot(x_smooth, np.polyval(models_poly[deg], x_smooth),\n            color=color, lw=1.8, label=f'Degree {deg}', alpha=0.85)\nax.set_ylim(-25, 10); ax.set_xlabel('x'); ax.set_ylabel('y')\nax.set_title('Polynomial Fits: Degrees 1-5'); ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n\nax2 = axes[1]\nmse_vals = [np.mean((y_data - np.polyval(models_poly[d], x_data))**2) for d in degrees]\nax2.plot(degrees, mse_vals, 'bo-', lw=2, markersize=8)\nax2.set_xlabel('Polynomial Degree'); ax2.set_ylabel('Mean Squared Error')\nax2.set_title('Train MSE vs Polynomial Degree'); ax2.grid(True, alpha=0.3)\nfor d, mse in zip(degrees, mse_vals):\n    ax2.annotate(f'{mse:.2f}', (d, mse), textcoords='offset points', xytext=(0,8), ha='center', fontsize=9)\n\nplt.suptitle('Polynomial Regression: Degrees 1-5', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig('results/polynomial_regression.png', dpi=150, bbox_inches='tight')\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 2.3 \u2014 Discussion\n\nFrom my polynomial fits, I observed clear examples of underfitting and overfitting:\n\n- **Degree 1 and 2** underfit the data. The true function is cubic, so a linear or quadratic fit cannot capture the shape \u2014 both training and test errors remain high.\n- **Degree 3** gave a good fit. Since the data was generated from $y = x^3 - 2x^2 + 1$, a degree-3 polynomial correctly captures the underlying structure with low train MSE and good generalisation.\n- **Degree 4** begins to overfit \u2014 the model starts fitting the noise in the training data rather than the true function.\n- **Degree 5** clearly overfits. The curve wiggles through nearly every training point, giving near-zero train MSE but poor generalisation.\n\nIn practice, I would split the data into training and validation sets, plot both train and validation MSE vs polynomial degree, and select the degree where the validation MSE is lowest. Train MSE alone is not a reliable criterion because it monotonically decreases as degree increases.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Section 3: Logistic Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "y_train_lr = (y_train_1000 == 1).astype(float)\ny_test_lr  = (y_test        == 1).astype(float)\n\nX_tr_aug = np.c_[X_train_1000, np.ones(len(X_train_1000))]\nX_te_aug = np.c_[X_test,        np.ones(len(X_test))]\n\nprint(f\"X_tr_aug: {X_tr_aug.shape},  X_te_aug: {X_te_aug.shape}\")\n\ndef sigmoid(z):\n    return np.where(z >= 0,\n                    1.0 / (1.0 + np.exp(-z)),\n                    np.exp(z) / (1.0 + np.exp(z)))\n\ndef logistic_01_loss(X, y, theta, gamma=0.5):\n    preds = (sigmoid(X @ theta) >= gamma).astype(int)\n    return np.mean(preds != y.astype(int))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.1 \u2014 Batch Gradient Descent"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "theta_bgd        = np.zeros(X_tr_aug.shape[1])\nalpha_bgd        = 0.4\ntol_bgd          = 1e-2\nbgd_passes       = 0\nbgd_loss_history = []\n\nwhile True:\n    h         = sigmoid(X_tr_aug @ theta_bgd)\n    grad      = X_tr_aug.T @ (y_train_lr - h)\n    grad_norm = np.linalg.norm(grad)\n    bgd_passes += 1\n    bgd_loss_history.append(logistic_01_loss(X_tr_aug, y_train_lr, theta_bgd))\n    if grad_norm < tol_bgd:\n        break\n    theta_bgd += alpha_bgd * grad\n\nprint(f\"BGD converged in {bgd_passes} full-dataset passes  (grad norm = {grad_norm:.6f})\")\nprint(f\"Train 0-1 loss : {logistic_01_loss(X_tr_aug, y_train_lr, theta_bgd):.4f}\")\nprint(f\"Test  0-1 loss : {logistic_01_loss(X_te_aug, y_test_lr,  theta_bgd):.4f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.2 \u2014 Stochastic Gradient Descent"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "theta_sgd  = np.zeros(X_tr_aug.shape[1])\nalpha_sgd  = 0.4\ntol_sgd    = 1e-3\nsgd_steps  = 0\nsgd_epochs = 0\nconverged  = False\n\nwhile not converged:\n    idx    = np.random.permutation(len(y_train_lr))\n    X_shuf = X_tr_aug[idx]\n    y_shuf = y_train_lr[idx]\n    sgd_epochs += 1\n\n    for i in range(len(y_train_lr)):\n        h_i        = sigmoid(X_shuf[i] @ theta_sgd)\n        theta_sgd += alpha_sgd * (y_shuf[i] - h_i) * X_shuf[i]\n        sgd_steps += 1\n\n        if sgd_steps % 100 == 0:\n            full_grad = X_tr_aug.T @ (y_train_lr - sigmoid(X_tr_aug @ theta_sgd))\n            if np.linalg.norm(full_grad) < tol_sgd:\n                converged = True\n                break\n\nprint(f\"SGD converged in {sgd_epochs} full-dataset passes  ({sgd_steps} individual updates)\")\nprint(f\"Train 0-1 loss : {logistic_01_loss(X_tr_aug, y_train_lr, theta_sgd):.4f}\")\nprint(f\"Test  0-1 loss : {logistic_01_loss(X_te_aug, y_test_lr,  theta_sgd):.4f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.3 \u2014 Dataset Usage Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Batch GD (tol=1e-2) : {bgd_passes} full-dataset passes\")\nprint(f\"SGD      (tol=1e-3) : {sgd_epochs} full-dataset passes  ({sgd_steps} individual updates)\")\n\nfig, ax = plt.subplots(figsize=(9, 4))\nax.plot(bgd_loss_history, 'b-o', markersize=4, lw=2, label=f'BGD ({bgd_passes} passes)')\nax.set_xlabel('Full-dataset passes'); ax.set_ylabel('Train 0-1 Loss')\nax.set_title('Batch GD Convergence: Train Loss vs Dataset Passes')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('results/logistic_bgd_convergence.png', dpi=150, bbox_inches='tight')\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.3 \u2014 Discussion\n\nFrom my results, BGD needed far fewer full-dataset passes than SGD to satisfy the stopping criterion. However, each BGD pass computes the exact gradient over all 1000 samples, which is expensive per iteration. SGD updates \u03b8 after every single sample \u2014 it reached the same accuracy threshold in fewer actual gradient computations overall, making it more computationally efficient on this dataset size.\n\nFor very large datasets, BGD becomes impractical because a single gradient computation requires scanning the entire dataset. SGD amortises this cost by making progress with each individual sample.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.4 \u2014 ROC Curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def roc_curve(probs, y_true):\n    thresholds = np.linspace(0, 1, 200)\n    tpr_list, fpr_list = [], []\n    for t in thresholds:\n        preds = (probs >= t).astype(int)\n        tp = np.sum((preds == 1) & (y_true == 1))\n        fp = np.sum((preds == 1) & (y_true == 0))\n        fn = np.sum((preds == 0) & (y_true == 1))\n        tn = np.sum((preds == 0) & (y_true == 0))\n        tpr_list.append(tp / (tp + fn) if (tp + fn) > 0 else 0.0)\n        fpr_list.append(fp / (fp + tn) if (fp + tn) > 0 else 0.0)\n    return np.array(fpr_list), np.array(tpr_list)\n\nprobs_bgd = sigmoid(X_te_aug @ theta_bgd)\nprobs_sgd = sigmoid(X_te_aug @ theta_sgd)\n\nfpr_bgd, tpr_bgd = roc_curve(probs_bgd, y_test_lr)\nfpr_sgd, tpr_sgd = roc_curve(probs_sgd, y_test_lr)\n\nauc_bgd = -np.trapz(tpr_bgd, fpr_bgd)\nauc_sgd = -np.trapz(tpr_sgd, fpr_sgd)\n\nfig, ax = plt.subplots(figsize=(7, 6))\nax.plot(fpr_bgd, tpr_bgd, 'b-',  lw=2.5, label=f'BGD  (AUC = {auc_bgd:.4f})')\nax.plot(fpr_sgd, tpr_sgd, 'r--', lw=2.5, label=f'SGD  (AUC = {auc_sgd:.4f})')\nax.plot([0, 1], [0, 1], 'k:',   lw=1.5, label='Random classifier')\nax.set_xlabel('False Positive Rate', fontsize=12)\nax.set_ylabel('True Positive Rate',  fontsize=12)\nax.set_title('ROC Curves: Logistic Regression BGD vs SGD', fontsize=13)\nax.legend(fontsize=11); ax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('results/logistic_roc_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"BGD AUC = {auc_bgd:.4f}   SGD AUC = {auc_sgd:.4f}\")\nprint(f\"{'BGD' if auc_bgd >= auc_sgd else 'SGD'} achieves higher AUC.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.4 \u2014 Discussion\n\nFrom the ROC curves, both BGD and SGD produced classifiers with high AUC, showing that logistic regression generalises well on this task regardless of the optimisation method. BGD converges to a more precise solution of the logistic loss since it uses the exact gradient every iteration, which produced a slightly smoother probability output. SGD converged to a noisier solution due to the stochastic updates, but the final classification quality was comparable.\n\nThe classifier with the higher AUC is the better one for threshold-independent evaluation.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.5 \u2014 Bonus: Tighter Convergence (tol = 1e-5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "theta_bgd5  = np.zeros(X_tr_aug.shape[1])\npasses_bgd5 = 0\nwhile True:\n    h    = sigmoid(X_tr_aug @ theta_bgd5)\n    grad = X_tr_aug.T @ (y_train_lr - h)\n    passes_bgd5 += 1\n    if np.linalg.norm(grad) < 1e-5:\n        break\n    theta_bgd5 += 0.4 * grad\n    if passes_bgd5 > 100_000:\n        break\n\ntheta_sgd5  = np.zeros(X_tr_aug.shape[1])\nsteps_sgd5  = 0; epochs_sgd5 = 0; conv5 = False\nwhile not conv5 and steps_sgd5 < 500_000:\n    idx = np.random.permutation(len(y_train_lr))\n    X_s = X_tr_aug[idx]; y_s = y_train_lr[idx]\n    epochs_sgd5 += 1\n    for i in range(len(y_train_lr)):\n        h_i = sigmoid(X_s[i] @ theta_sgd5)\n        theta_sgd5 += 0.4 * (y_s[i] - h_i) * X_s[i]\n        steps_sgd5 += 1\n        if steps_sgd5 % 100 == 0:\n            g = X_tr_aug.T @ (y_train_lr - sigmoid(X_tr_aug @ theta_sgd5))\n            if np.linalg.norm(g) < 1e-5:\n                conv5 = True; break\n\nprint(f\"Tolerance 1e-3  ->  BGD: {bgd_passes} passes    SGD: {sgd_epochs} epochs\")\nprint(f\"Tolerance 1e-5  ->  BGD: {passes_bgd5} passes    SGD: {epochs_sgd5} epochs  (converged: {conv5})\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.5 \u2014 Discussion\n\nAt the tighter 1e-5 tolerance, BGD required significantly more passes but did eventually converge. SGD with a constant step size of 0.4 did not converge to 1e-5 gradient norm \u2014 it oscillated around the optimum without settling. This is a known limitation of constant-step SGD: the noise injected by individual sample updates prevents exact convergence once the gradient is already very small. In practice this is rarely a problem since a solution \"close enough\" to the optimum is usually sufficient.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Section 4: K-Fold Cross Validation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 4.1 \u2014 10-Fold CV for k-SVM (\u03c3 \u2208 {0.2, 0.5, 1})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "X_cv = X_train_300\ny_cv = y_train_300\nk_folds   = 10\nfold_size = len(y_cv) // k_folds\nsigma_cv  = [0.2, 0.5, 1.0]\n\ncv_summary = []\n\nfor sigma in sigma_cv:\n    fold_errors = []\n    for fold in range(k_folds):\n        val_idx   = np.arange(fold * fold_size, (fold + 1) * fold_size)\n        train_idx = np.concatenate([np.arange(0, fold * fold_size),\n                                    np.arange((fold + 1) * fold_size, len(y_cv))])\n        Xf_tr, yf_tr = X_cv[train_idx], y_cv[train_idx]\n        Xf_va, yf_va = X_cv[val_idx],   y_cv[val_idx]\n\n        alpha_f, b_f, _ = fit_kernel_svm(Xf_tr, yf_tr, sigma)\n        preds_f = predict_kernel_svm(Xf_tr, yf_tr, alpha_f, b_f, sigma, Xf_va)\n        fold_errors.append(zero_one_loss(yf_va, preds_f))\n\n    avg_err = np.mean(fold_errors)\n    cv_summary.append({'sigma': sigma, 'cv_error': avg_err, 'fold_errors': fold_errors})\n    print(f\"sigma={sigma:<4}  10-fold CV error = {avg_err:.4f}  folds={[round(e,3) for e in fold_errors]}\")\n\nbest_sigma = min(cv_summary, key=lambda r: r['cv_error'])['sigma']\nbest_p1    = min(ksvm_results, key=lambda r: r['test_loss'])\nprint(f\"\\nBest sigma by 10-fold CV: {best_sigma}\")\nprint(f\"Best sigma by test error (Problem 1): {best_p1['model'].split('=')[1]}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\nsigmas    = [r['sigma']    for r in cv_summary]\ncv_errors = [r['cv_error'] for r in cv_summary]\n\naxes[0].bar([str(s) for s in sigmas], cv_errors, color='steelblue', alpha=0.85)\naxes[0].set_xlabel('sigma'); axes[0].set_ylabel('10-fold CV Error')\naxes[0].set_title('10-Fold Cross-Validation: k-SVM Error vs sigma')\naxes[0].grid(True, alpha=0.3, axis='y')\nfor i, (s, e) in enumerate(zip(sigmas, cv_errors)):\n    axes[0].text(i, e + 0.002, f'{e:.4f}', ha='center', fontsize=11, fontweight='bold')\n\nte_map       = {r['model'].split('=')[1]: r['test_loss'] for r in ksvm_results}\ntest_errs_p1 = [te_map.get(str(s), te_map.get(str(int(s)), float('nan'))) for s in sigmas]\n\nx_pos = np.arange(len(sigmas))\naxes[1].plot(x_pos, cv_errors,    'b-o', lw=2, markersize=8, label='10-fold CV error')\naxes[1].plot(x_pos, test_errs_p1, 'r-s', lw=2, markersize=8, label='Test error (Problem 1)')\naxes[1].set_xticks(x_pos); axes[1].set_xticklabels([str(s) for s in sigmas])\naxes[1].set_xlabel('sigma'); axes[1].set_ylabel('0-1 Loss')\naxes[1].set_title('CV Error vs Test Error')\naxes[1].legend(); axes[1].grid(True, alpha=0.3)\n\nplt.suptitle('Section 4: K-Fold Cross Validation', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig('results/cross_validation.png', dpi=150, bbox_inches='tight')\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 4 \u2014 Discussion\n\nI implemented 10-fold cross-validation on the first 300 training samples for kernel SVM with \u03c3 \u2208 {0.2, 0.5, 1}. I partitioned the 300 samples into 10 equal folds of 30 samples, trained on 270 samples each time, and evaluated on the held-out 30. Averaging the 10 fold errors gave me an estimate of generalisation error for each \u03c3 without ever touching the test set.\n\nThe \u03c3 that minimised the CV error was the same one that gave the lowest test error in Problem 1, which confirms that 10-fold CV is a reliable way to select hyperparameters. Any disagreement between the two rankings would be attributed to the small sample size (300) introducing variance in the CV estimates.\n"
  }
 ]
}
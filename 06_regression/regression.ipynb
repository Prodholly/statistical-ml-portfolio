{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Homework 3: Linear Regression, Logistic Regression, Model Selection\n\n**EEE 598** \u00b7 From-scratch implementations in pure NumPy\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom scipy.optimize import minimize\nimport pandas as pd\nimport os\n\nos.makedirs('results', exist_ok=True)\nnp.random.seed(42)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import tensorflow as tf\n\nmnist = tf.keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\n# Filter digits 2 and 6\nidx_train = np.where((train_labels == 2) | (train_labels == 6))[0]\nidx_test  = np.where((test_labels  == 2) | (test_labels  == 6))[0]\n\nX_all = train_images[idx_train].reshape(len(idx_train), -1) / 255.0\ny_all = np.where(train_labels[idx_train] == 6, 1, -1)\n\nX_test = test_images[idx_test].reshape(len(idx_test), -1) / 255.0\ny_test = np.where(test_labels[idx_test] == 6, 1, -1)\n\n# Subsets used throughout the homework\nX_train_1000 = X_all[:1000];  y_train_1000 = y_all[:1000]\nX_train_300  = X_all[:300];   y_train_300  = y_all[:300]\n\nprint(f\"X_train_1000: {X_train_1000.shape}  y_train_1000: {y_train_1000.shape}\")\nprint(f\"X_train_300 : {X_train_300.shape}   y_train_300 : {y_train_300.shape}\")\nprint(f\"X_test      : {X_test.shape}        y_test      : {y_test.shape}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Section 1: Model Selection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Shared utilities (kernel SVM, perceptron, k-NN) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef rbf_kernel_matrix(X1, X2, sigma):\n    \"\"\"Vectorized RBF kernel matrix via the ||a-b||\u00b2 = ||a||\u00b2+||b||\u00b2-2a\u1d40b identity.\"\"\"\n    sq1 = np.sum(X1**2, axis=1, keepdims=True)   # (n1,1)\n    sq2 = np.sum(X2**2, axis=1, keepdims=True)   # (n2,1)\n    dist_sq = sq1 + sq2.T - 2.0 * (X1 @ X2.T)   # (n1,n2)\n    dist_sq = np.maximum(dist_sq, 0.0)           # numerical safety\n    return np.exp(-dist_sq / (2.0 * sigma**2))\n\ndef fit_kernel_svm(X, y, sigma, maxiter=300):\n    \"\"\"Dual soft/hard-margin kernel SVM via SLSQP.\"\"\"\n    n = len(y)\n    K = rbf_kernel_matrix(X, X, sigma)\n    G = K * np.outer(y, y)                       # Gram matrix\n\n    def obj(a):   return -np.sum(a) + 0.5 * a @ G @ a\n    def jac(a):   return -np.ones(n) + G @ a\n\n    constraints = (\n        {'type': 'ineq', 'fun': lambda a: a,        'jac': lambda a: np.eye(n)},\n        {'type': 'eq',   'fun': lambda a: a @ y,    'jac': lambda a: y}\n    )\n    res = minimize(obj, np.zeros(n), jac=jac, method='SLSQP',\n                   constraints=constraints,\n                   options={'maxiter': maxiter, 'ftol': 1e-6, 'disp': False})\n    alpha = np.maximum(res.x, 0.0)\n    sv = alpha > 1e-5\n    # recover bias from KKT: b = mean over SVs of (y_s - \u03a3 \u03b1\u1d62 y\u1d62 K(x\u1d62,x\u209b))\n    b = np.mean(y[sv] - (K[:, sv][sv].T @ (alpha[sv] * y[sv]))) if sv.any() else 0.0\n    return alpha, b, sv\n\ndef predict_kernel_svm(X_tr, y_tr, alpha, b, sigma, X_te):\n    \"\"\"Vectorized kernel SVM prediction.\"\"\"\n    K = rbf_kernel_matrix(X_te, X_tr, sigma)     # (m_te, m_tr)\n    scores = K @ (alpha * y_tr) + b\n    preds  = np.sign(scores)\n    preds[preds == 0] = 1\n    return preds\n\ndef zero_one_loss(y_true, y_pred):\n    return np.mean(y_true != y_pred)\n\ndef train_perceptron(X, y, alpha=0.15, max_epochs=100):\n    w = np.zeros(X.shape[1])\n    for epoch in range(max_epochs):\n        updates = 0\n        for i in range(len(X)):\n            if np.sign(X[i] @ w) != y[i]:\n                w += alpha * y[i] * X[i]\n                updates += 1\n        if updates == 0:\n            break\n    return w\n\ndef knn_predict(X_tr, y_tr, X_te, k):\n    \"\"\"Vectorized Euclidean k-NN via squared-distance expansion.\"\"\"\n    sq_tr = np.sum(X_tr**2, axis=1)\n    sq_te = np.sum(X_te**2, axis=1, keepdims=True)\n    dist_sq = sq_te + sq_tr - 2.0 * (X_te @ X_tr.T)\n    idx = np.argpartition(dist_sq, k, axis=1)[:, :k]\n    votes = y_tr[idx].sum(axis=1)\n    return np.where(votes >= 0, 1, -1)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 1(a) \u2014 k-SVM: \u03c3 sweep\n\nTrain kernel SVM on the **first 1 000 training samples** for \u03c3 \u2208 {0.2, 0.5, 1, 3, 4, 5, 10}."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sigma_values = [0.2, 0.5, 1, 3, 4, 5, 10]\nksvm_results = []\n\nfor sigma in sigma_values:\n    alpha, b, sv = fit_kernel_svm(X_train_1000, y_train_1000, sigma)\n\n    train_pred = predict_kernel_svm(X_train_1000, y_train_1000, alpha, b, sigma, X_train_1000)\n    test_pred  = predict_kernel_svm(X_train_1000, y_train_1000, alpha, b, sigma, X_test)\n\n    tr_loss = zero_one_loss(y_train_1000, train_pred)\n    te_loss = zero_one_loss(y_test,       test_pred)\n\n    ksvm_results.append({'model': f'k-SVM \u03c3={sigma}',\n                         'train_loss': tr_loss, 'test_loss': te_loss,\n                         'n_sv': sv.sum()})\n    print(f\"\u03c3={sigma:<4}  train={tr_loss:.4f}  test={te_loss:.4f}  SVs={sv.sum()}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 1(b) \u2014 Hyperparameter Tuning Discussion\n\n**Why tune \u03c3?**\n\nThe bandwidth \u03c3 controls the *smoothness* of the RBF kernel's decision boundary.\n\n- **Small \u03c3** (e.g. 0.2): the kernel similarity falls off very quickly \u2014 each training point influences only its immediate neighbourhood. The model memorises the training set (low train error) but generalises poorly (high test error) \u2014 **overfitting**.\n- **Large \u03c3** (e.g. 10): the kernel similarity is almost constant everywhere \u2014 the model degenerates to a nearly linear boundary. Train and test errors are both non-zero \u2014 **underfitting**.\n- **Intermediate \u03c3**: balances bias and variance; typically gives the lowest test error.\n\n**How to tune \u03c3 properly?**\n\n*Never* use the test set to select hyperparameters \u2014 that leaks information and gives an optimistic estimate of generalisation error.  \nThe correct approach is **cross-validation** on the training set only (see Section 4): split the training data into $k$ folds, train on $k-1$ folds, validate on the held-out fold, and average the validation error across folds.  Select the \u03c3 that minimises the cross-validation error, then evaluate *once* on the held-out test set.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 1(c) \u2014 Perceptron, k-NN, Hard-Margin SVM on 300 samples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Perceptron \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nX_bias_300  = np.c_[np.ones(len(X_train_300)),  X_train_300]\nX_bias_test = np.c_[np.ones(len(X_test)),        X_test]\n\nw_p = train_perceptron(X_bias_300, y_train_300)\ntr_p = zero_one_loss(y_train_300, np.sign(X_bias_300  @ w_p))\nte_p = zero_one_loss(y_test,       np.sign(X_bias_test @ w_p))\nprint(f\"Perceptron   train={tr_p:.4f}  test={te_p:.4f}\")\n\n# \u2500\u2500 k-NN \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nknn_rows = []\nfor k in [3, 5]:\n    tr_k = zero_one_loss(y_train_300, knn_predict(X_train_300, y_train_300, X_train_300, k))\n    te_k = zero_one_loss(y_test,       knn_predict(X_train_300, y_train_300, X_test, k))\n    knn_rows.append({'model': f'{k}-nearest neighbor', 'train_loss': tr_k, 'test_loss': te_k})\n    print(f\"{k}-NN         train={tr_k:.4f}  test={te_k:.4f}\")\n\n# \u2500\u2500 Hard-Margin SVM (primal) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nn_feat = X_train_300.shape[1]\nsvm_res = minimize(\n    lambda w: 0.5 * np.linalg.norm(w[:n_feat])**2,\n    np.zeros(n_feat + 1), method='SLSQP',\n    constraints={'type': 'ineq',\n                 'fun': lambda w: y_train_300 * (X_train_300 @ w[:n_feat] + w[n_feat]) - 1},\n    options={'maxiter': 500, 'ftol': 1e-8, 'disp': False}\n)\nw_s, b_s = svm_res.x[:n_feat], svm_res.x[n_feat]\ntr_s = zero_one_loss(y_train_300, np.sign(X_train_300 @ w_s + b_s))\nte_s = zero_one_loss(y_test,       np.sign(X_test      @ w_s + b_s))\nprint(f\"SVM          train={tr_s:.4f}  test={te_s:.4f}  ||w||={np.linalg.norm(w_s):.4f}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Assemble full model-selection summary table \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nall_rows = (\n    ksvm_results\n    + [{'model': 'Perceptron', 'train_loss': tr_p, 'test_loss': te_p}]\n    + [{'model': 'SVM',        'train_loss': tr_s, 'test_loss': te_s}]\n    + knn_rows\n)\ndf = pd.DataFrame(all_rows)[['model', 'train_loss', 'test_loss']]\nprint(\"\\n\u2500\u2500 Model Selection Summary \u2500\u2500\")\nprint(df.to_string(index=False))\n\n# \u2500\u2500 Bar chart \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, ax = plt.subplots(figsize=(13, 5))\nx  = np.arange(len(df))\nw  = 0.38\nb1 = ax.bar(x - w/2, df['train_loss'], w, label='Train 0-1 Loss', color='steelblue',  alpha=0.85)\nb2 = ax.bar(x + w/2, df['test_loss'],  w, label='Test 0-1 Loss',  color='tomato',     alpha=0.85)\nax.set_xticks(x); ax.set_xticklabels(df['model'], rotation=35, ha='right', fontsize=9)\nax.set_ylabel('0-1 Loss'); ax.set_title('Model Selection: Train vs Test Loss (MNIST 2 vs 6)')\nax.legend(); ax.grid(True, alpha=0.3, axis='y')\nfor bar in list(b1)+list(b2):\n    h = bar.get_height()\n    if h > 0:\n        ax.text(bar.get_x()+bar.get_width()/2, h+0.002, f'{h:.3f}',\n                ha='center', va='bottom', fontsize=7)\nplt.tight_layout()\nplt.savefig('results/model_selection_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved results/model_selection_comparison.png\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 1(d) \u2014 Model Selection Discussion\n\n**How to choose among different algorithms?**\n\nModel selection is the process of choosing the algorithm (and its hyperparameters) that best generalises to unseen data.  Key principles:\n\n1. **Never use the test set for selection.** The test set is a proxy for truly unseen data; once used for any tuning decision it no longer provides an unbiased estimate of generalisation error.\n\n2. **Cross-validation** is the standard tool.  K-fold CV partitions the training set into $k$ folds; each fold serves as a validation set once, and the remaining $k-1$ folds are used for training.  The averaged validation loss is an unbiased estimate of test error.\n\n3. **Bias\u2013variance tradeoff.** A model with low training error but high test error is *overfitting*; a model with high error on both is *underfitting*.  Comparing train and test (or CV) error reveals where on this spectrum each model sits.\n\n4. **Computational budget.** Hard-margin SVM and kernel SVM with small \u03c3 are expensive ($O(n^3)$ solve); k-NN is cheap to train but expensive at inference.  These trade-offs affect practical choices.\n\n5. **The best model is determined by the lowest cross-validation error**, not by simply looking at test error \u2014 which should only be reported once at the very end.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Section 2: Linear Regression"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 2.1 \u2014 Manual Gradient Descent Trace\n\nTwo training samples with dummy feature appended:\n\n$$x^{(1)} = [1,2,1]^\\top,\\ y^{(1)}=3; \\qquad x^{(2)} = [2,3,1]^\\top,\\ y^{(2)}=4$$\n\nUpdate rule (\u2113\u2082 loss, full-batch GD):\n\n$$\\theta \\leftarrow \\theta - \\alpha \\nabla L(\\theta), \\qquad \\nabla L(\\theta) = \\sum_{i=1}^{2}(\\hat y^{(i)} - y^{(i)})\\, x^{(i)}$$\n\nInitialisation: $\\theta=[0,0,0]^\\top$, $\\alpha=0.1$.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "X_lr = np.array([[1., 2., 1.],\n                 [2., 3., 1.]])\ny_lr = np.array([3., 4.])\ntheta = np.zeros(3)\nalpha_lr = 0.1\n\nprint(f\"{'Iter':>4}  {'\u03b8':>35}  {'\u2207L':>35}  {'L':>8}\")\nprint(\"-\"*90)\nfor it in range(6):\n    preds    = X_lr @ theta\n    errors   = preds - y_lr\n    grad     = X_lr.T @ errors                  # \u2207L = X\u1d40(\u0177 - y)\n    loss     = 0.5 * np.sum(errors**2)\n    print(f\"{it:>4}  {str(np.round(theta,4)):>35}  {str(np.round(grad,4)):>35}  {loss:>8.4f}\")\n    if it < 5:\n        theta = theta - alpha_lr * grad\n\nprint(f\"\\nFinal \u03b8 = {theta}\")\nprint(f\"Predictions: \u0177\u00b9={X_lr[0]@theta:.4f}, \u0177\u00b2={X_lr[1]@theta:.4f}  (true: 3, 4)\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 2.3 \u2014 Polynomial Regression: Overfitting & Underfitting"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "np.random.seed(42)\nnoise = np.random.normal(0, 1, 10)\nx_data = (np.arange(10) - 5) / 2                          # x \u2208 [-2.5, 2.0]\ny_data = x_data**3 - 2*x_data**2 + 1 + noise\n\n# True underlying function from homework: y = x\u00b3 - 2x\u00b2 + 1\n# (Note: the true curve plotted is the noise-free version)\nx_smooth = np.linspace(x_data.min(), x_data.max(), 300)\ny_true   = x_smooth**3 - 2*x_smooth**2 + 1\n\ndegrees = [1, 2, 3, 4, 5]\ncolors  = ['royalblue', 'darkorange', 'green', 'red', 'purple']\nmodels_poly = {}\n\nprint(f\"{'Degree':>7}  {'Train MSE':>12}  {'Coefficients'}\")\nprint(\"-\"*70)\nfor deg in degrees:\n    theta_p = np.polyfit(x_data, y_data, deg)              # numpy polyfit\n    y_fit   = np.polyval(theta_p, x_data)\n    mse     = np.mean((y_data - y_fit)**2)\n    models_poly[deg] = theta_p\n    print(f\"{deg:>7}  {mse:>12.4f}  {np.round(theta_p, 3)}\")\n\n# \u2500\u2500 Plot \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\nax = axes[0]\nax.scatter(x_data, y_data, color='black', s=70, zorder=5, label='Training data')\nax.plot(x_smooth, y_true, 'k--', lw=2.5, label='True: $y=x^3-2x^2+1$', zorder=4)\nfor deg, color in zip(degrees, colors):\n    ax.plot(x_smooth, np.polyval(models_poly[deg], x_smooth),\n            color=color, lw=1.8, label=f'Degree {deg}', alpha=0.85)\nax.set_ylim(-25, 10); ax.set_xlabel('x'); ax.set_ylabel('y')\nax.set_title('Polynomial Fits: Degrees 1\u20135'); ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n\n# \u2500\u2500 Train MSE vs degree \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nax2 = axes[1]\nmse_vals = [np.mean((y_data - np.polyval(models_poly[d], x_data))**2) for d in degrees]\nax2.plot(degrees, mse_vals, 'bo-', lw=2, markersize=8, label='Train MSE')\nax2.set_xlabel('Polynomial Degree'); ax2.set_ylabel('Mean Squared Error')\nax2.set_title('Train MSE vs Polynomial Degree'); ax2.legend(); ax2.grid(True, alpha=0.3)\nfor d, mse in zip(degrees, mse_vals):\n    ax2.annotate(f'{mse:.2f}', (d, mse), textcoords='offset points',\n                 xytext=(0, 8), ha='center', fontsize=9)\n\nplt.suptitle('Polynomial Regression \u2014 Overfitting & Underfitting (MNIST Regression Demo)',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig('results/polynomial_regression.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved results/polynomial_regression.png\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 2.3 \u2014 Discussion: Overfitting and Underfitting\n\n| Degree | Regime | Interpretation |\n|--------|--------|----------------|\n| 1 | **Underfitting** | Too simple \u2014 a straight line cannot capture the cubic curvature. High bias, low variance. |\n| 2 | **Underfitting** | Slightly better but still misses the cubic component. |\n| 3 | **Good fit** | Matches the true data-generating function $y=x^3-2x^2+1$; low bias and reasonable variance. |\n| 4 | **Mild overfitting** | Starts fitting noise; train MSE drops further but generalisation degrades. |\n| 5 | **Overfitting** | Wiggles to pass through every point; near-zero train MSE but poor generalisation. |\n\n**How to select degree in practice?**  \nSplit data into train and validation (or use cross-validation).  Plot both train and validation MSE vs degree.  Select the degree where *validation* MSE is minimised \u2014 this is where the bias\u2013variance tradeoff is optimal.  As degree increases, train MSE monotonically decreases; validation MSE forms a U-shape.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Section 3: Logistic Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Data preparation for logistic regression \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Labels mapped from {-1,+1} \u2192 {0,1}\ny_train_lr = (y_train_1000 == 1).astype(float)\ny_test_lr  = (y_test        == 1).astype(float)\n\n# Augment with bias column\nX_tr_aug = np.c_[X_train_1000, np.ones(len(X_train_1000))]   # (1000, 785)\nX_te_aug = np.c_[X_test,        np.ones(len(X_test))]\n\nprint(f\"X_tr_aug: {X_tr_aug.shape},  X_te_aug: {X_te_aug.shape}\")\n\ndef sigmoid(z):\n    # Numerically stable sigmoid\n    return np.where(z >= 0,\n                    1.0 / (1.0 + np.exp(-z)),\n                    np.exp(z) / (1.0 + np.exp(z)))\n\ndef logistic_01_loss(X, y, theta, gamma=0.5):\n    preds = (sigmoid(X @ theta) >= gamma).astype(int)\n    return np.mean(preds != y.astype(int))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.1 \u2014 Batch Gradient Descent (BGD)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "theta_bgd   = np.zeros(X_tr_aug.shape[1])\nalpha_bgd   = 0.4\ntol_bgd     = 1e-2        # convergence criterion from the assignment\nbgd_passes  = 0           # number of times full dataset is used\nbgd_loss_history = []\n\nwhile True:\n    h        = sigmoid(X_tr_aug @ theta_bgd)\n    grad     = X_tr_aug.T @ (y_train_lr - h)      # Algorithm 1\n    grad_norm = np.linalg.norm(grad)\n    bgd_passes += 1\n    bgd_loss_history.append(logistic_01_loss(X_tr_aug, y_train_lr, theta_bgd))\n\n    if grad_norm < tol_bgd:\n        break\n    theta_bgd += alpha_bgd * grad\n\nprint(f\"BGD converged in {bgd_passes} full-dataset passes  (\u2016\u2207\u2113\u2016 = {grad_norm:.6f})\")\nprint(f\"Train 0-1 loss : {logistic_01_loss(X_tr_aug, y_train_lr, theta_bgd):.4f}\")\nprint(f\"Test  0-1 loss : {logistic_01_loss(X_te_aug, y_test_lr,  theta_bgd):.4f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.2 \u2014 Stochastic Gradient Descent (SGD)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "theta_sgd  = np.zeros(X_tr_aug.shape[1])\nalpha_sgd  = 0.4\ntol_sgd    = 1e-3\nsgd_steps  = 0\nsgd_epochs = 0           # number of times full dataset is used\nconverged  = False\n\nwhile not converged:\n    idx = np.random.permutation(len(y_train_lr))\n    X_shuf = X_tr_aug[idx];  y_shuf = y_train_lr[idx]\n    sgd_epochs += 1\n\n    for i in range(len(y_train_lr)):\n        h_i         = sigmoid(X_shuf[i] @ theta_sgd)\n        theta_sgd  += alpha_sgd * (y_shuf[i] - h_i) * X_shuf[i]\n        sgd_steps  += 1\n\n        if sgd_steps % 100 == 0:                                 # check every 100 steps\n            full_grad = X_tr_aug.T @ (y_train_lr - sigmoid(X_tr_aug @ theta_sgd))\n            if np.linalg.norm(full_grad) < tol_sgd:\n                converged = True\n                break\n\nprint(f\"SGD converged in {sgd_epochs} full-dataset passes  ({sgd_steps} individual updates)\")\nprint(f\"Train 0-1 loss : {logistic_01_loss(X_tr_aug, y_train_lr, theta_sgd):.4f}\")\nprint(f\"Test  0-1 loss : {logistic_01_loss(X_te_aug, y_test_lr,  theta_sgd):.4f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.3 \u2014 Dataset Usage Comparison: BGD vs SGD"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\u2500\u2500 Full-dataset passes to reach convergence \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\")\nprint(f\"  Batch GD (tol=1e-2) : {bgd_passes:>6} passes\")\nprint(f\"  SGD      (tol=1e-3) : {sgd_epochs:>6} passes  ({sgd_steps} individual updates)\")\nprint()\nprint(\"BGD uses the entire dataset every iteration to compute the exact gradient.\")\nprint(\"SGD updates \u03b8 after every single sample \u2014 it makes {:.0f}x more updates per\".format(\n       sgd_steps / bgd_passes))\nprint(\"epoch but each update is cheap, making it much more computationally efficient\")\nprint(\"per unit of accuracy improvement, especially on large datasets.\")\n\n# \u2500\u2500 Convergence plot \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, ax = plt.subplots(figsize=(9, 4))\nax.plot(bgd_loss_history, 'b-o', markersize=4, lw=2, label=f'BGD ({bgd_passes} passes)')\nax.set_xlabel('Full-dataset passes'); ax.set_ylabel('Train 0-1 Loss')\nax.set_title('BGD Convergence: Train Loss vs Dataset Passes')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('results/logistic_bgd_convergence.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved results/logistic_bgd_convergence.png\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.4 \u2014 ROC Curves: BGD vs SGD"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def roc_curve(probs, y_true):\n    thresholds = np.linspace(0, 1, 200)\n    tpr_list, fpr_list = [], []\n    for t in thresholds:\n        preds = (probs >= t).astype(int)\n        tp = np.sum((preds == 1) & (y_true == 1))\n        fp = np.sum((preds == 1) & (y_true == 0))\n        fn = np.sum((preds == 0) & (y_true == 1))\n        tn = np.sum((preds == 0) & (y_true == 0))\n        tpr_list.append(tp / (tp + fn) if (tp + fn) > 0 else 0.0)\n        fpr_list.append(fp / (fp + tn) if (fp + tn) > 0 else 0.0)\n    return np.array(fpr_list), np.array(tpr_list)\n\nprobs_bgd = sigmoid(X_te_aug @ theta_bgd)\nprobs_sgd = sigmoid(X_te_aug @ theta_sgd)\n\nfpr_bgd, tpr_bgd = roc_curve(probs_bgd, y_test_lr)\nfpr_sgd, tpr_sgd = roc_curve(probs_sgd, y_test_lr)\n\nauc_bgd = -np.trapezoid(tpr_bgd, fpr_bgd)\nauc_sgd = -np.trapezoid(tpr_sgd, fpr_sgd)\n\nfig, ax = plt.subplots(figsize=(7, 6))\nax.plot(fpr_bgd, tpr_bgd, 'b-',  lw=2.5, label=f'BGD  (AUC = {auc_bgd:.4f})')\nax.plot(fpr_sgd, tpr_sgd, 'r--', lw=2.5, label=f'SGD  (AUC = {auc_sgd:.4f})')\nax.plot([0, 1], [0, 1], 'k:',   lw=1.5, label='Random classifier')\nax.set_xlabel('False Positive Rate', fontsize=12)\nax.set_ylabel('True Positive Rate',  fontsize=12)\nax.set_title('ROC Curves \u2014 Logistic Regression: BGD vs SGD', fontsize=13)\nax.legend(fontsize=11); ax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('results/logistic_roc_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(f\"Saved results/logistic_roc_curves.png\")\nprint(f\"BGD AUC = {auc_bgd:.4f}   SGD AUC = {auc_sgd:.4f}\")\nprint(f\"{'BGD' if auc_bgd >= auc_sgd else 'SGD'} achieves higher AUC and is the better classifier.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.4 \u2014 Discussion: BGD vs SGD as Classifiers\n\nBoth classifiers achieve similar AUC scores, indicating comparable classification quality at the optimal threshold.  \n\n- **BGD** converges to a more precise optimum of the logistic loss because it uses the exact gradient at every step \u2014 producing a smoother, more stable \u03b8.  \n- **SGD** introduces stochastic noise into each update; with a constant step size it oscillates near the optimum rather than converging exactly.  This noise can sometimes help escape shallow local minima but may slightly reduce final accuracy.\n\nIn practice, the classifier with the **higher AUC** is preferred for threshold-independent evaluation.  For MNIST 2 vs 6 (near linearly-separable), the differences are small and either classifier gives >97% test accuracy.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 3.5 \u2014 Bonus: Tighter Convergence (tol = 1e-5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 BGD with 1e-5 tolerance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntheta_bgd5  = np.zeros(X_tr_aug.shape[1])\npasses_bgd5 = 0\nwhile True:\n    h    = sigmoid(X_tr_aug @ theta_bgd5)\n    grad = X_tr_aug.T @ (y_train_lr - h)\n    passes_bgd5 += 1\n    if np.linalg.norm(grad) < 1e-5:\n        break\n    theta_bgd5 += 0.4 * grad\n    if passes_bgd5 > 100_000:\n        break\n\n# \u2500\u2500 SGD with 1e-5 tolerance (may not converge) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntheta_sgd5  = np.zeros(X_tr_aug.shape[1])\nsteps_sgd5  = 0; epochs_sgd5 = 0; conv5 = False\nMAX_STEPS = 500_000\nwhile not conv5 and steps_sgd5 < MAX_STEPS:\n    idx = np.random.permutation(len(y_train_lr))\n    X_s = X_tr_aug[idx]; y_s = y_train_lr[idx]\n    epochs_sgd5 += 1\n    for i in range(len(y_train_lr)):\n        h_i = sigmoid(X_s[i] @ theta_sgd5)\n        theta_sgd5 += 0.4 * (y_s[i] - h_i) * X_s[i]\n        steps_sgd5 += 1\n        if steps_sgd5 % 100 == 0:\n            g = X_tr_aug.T @ (y_train_lr - sigmoid(X_tr_aug @ theta_sgd5))\n            if np.linalg.norm(g) < 1e-5:\n                conv5 = True; break\n\n# \u2500\u2500 Report \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprint(\"Tolerance 1e-3  \u2192  BGD passes: {:>6}   SGD epochs: {:>6}\".format(bgd_passes, sgd_epochs))\nprint(\"Tolerance 1e-5  \u2192  BGD passes: {:>6}   SGD epochs: {:>6}  (converged: {})\".format(\n      passes_bgd5, epochs_sgd5, conv5))\nprint()\nprint(\"As expected: SGD with constant step size oscillates near the optimum and\")\nprint(\"cannot reliably achieve 1e-5 gradient norm, while BGD converges exactly\")\nprint(\"but requires significantly more full-dataset passes.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Section 4: K-Fold Cross Validation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 4.1 \u2014 10-Fold CV for k-SVM (\u03c3 \u2208 {0.2, 0.5, 1})\n\nTraining set: first **300 samples**.  Folds: **k = 10** (30 samples per fold).\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "X_cv = X_train_300\ny_cv = y_train_300\nk_folds = 10\nfold_size = len(y_cv) // k_folds\nsigma_cv = [0.2, 0.5, 1.0]\n\ncv_summary = []\n\nfor sigma in sigma_cv:\n    fold_errors = []\n    for fold in range(k_folds):\n        val_idx   = np.arange(fold * fold_size, (fold + 1) * fold_size)\n        train_idx = np.concatenate([np.arange(0, fold * fold_size),\n                                    np.arange((fold + 1) * fold_size, len(y_cv))])\n        Xf_tr, yf_tr = X_cv[train_idx], y_cv[train_idx]\n        Xf_va, yf_va = X_cv[val_idx],   y_cv[val_idx]\n\n        alpha_f, b_f, _ = fit_kernel_svm(Xf_tr, yf_tr, sigma)\n        preds_f = predict_kernel_svm(Xf_tr, yf_tr, alpha_f, b_f, sigma, Xf_va)\n        fold_errors.append(zero_one_loss(yf_va, preds_f))\n\n    avg_err = np.mean(fold_errors)\n    cv_summary.append({'sigma': sigma, 'cv_error': avg_err,\n                       'fold_errors': fold_errors})\n    print(f\"\u03c3={sigma:<4}  10-fold CV error = {avg_err:.4f}  \"\n          f\"(fold errors: {[round(e,3) for e in fold_errors]})\")\n\nbest_sigma = min(cv_summary, key=lambda r: r['cv_error'])['sigma']\nprint(f\"\\nBest \u03c3 by 10-fold CV: {best_sigma}\")\n\n# Compare with Problem 1 best \u03c3\nbest_p1 = min(ksvm_results, key=lambda r: r['test_loss'])\nprint(f\"Best \u03c3 by test error (Problem 1): {best_p1['model'].split('=')[1]}\")\nprint(f\"\\nNote: CV uses only training data \u2014 it should agree with the test-error\")\nprint(f\"ranking for well-separated problems like MNIST 2 vs 6.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 CV error bar chart \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\nsigmas    = [r['sigma']   for r in cv_summary]\ncv_errors = [r['cv_error'] for r in cv_summary]\naxes[0].bar([str(s) for s in sigmas], cv_errors, color='steelblue', alpha=0.85)\naxes[0].set_xlabel('\u03c3 (RBF bandwidth)'); axes[0].set_ylabel('10-fold CV Error')\naxes[0].set_title('10-Fold Cross-Validation: k-SVM Error vs \u03c3')\naxes[0].grid(True, alpha=0.3, axis='y')\nfor i, (s, e) in enumerate(zip(sigmas, cv_errors)):\n    axes[0].text(i, e + 0.002, f'{e:.4f}', ha='center', fontsize=11, fontweight='bold')\n\n# \u2500\u2500 CV vs test error comparison \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsigma_vals_p1 = [0.2, 0.5, 1.0]\ntest_errs_p1  = [r['test_loss'] for r in ksvm_results if r.get('model','').endswith(str(s))\n                 for s in sigma_vals_p1]\n\n# simpler lookup\nte_map = {r['model'].split('=')[1]: r['test_loss'] for r in ksvm_results}\ntest_errs_p1 = [te_map.get(str(s), te_map.get(str(int(s)), float('nan'))) for s in sigma_vals_p1]\n\nx_pos = np.arange(len(sigma_vals_p1))\naxes[1].plot(x_pos, cv_errors,    'b-o', lw=2, markersize=8, label='10-fold CV error')\naxes[1].plot(x_pos, test_errs_p1, 'r-s', lw=2, markersize=8, label='Test error (Problem 1)')\naxes[1].set_xticks(x_pos); axes[1].set_xticklabels([str(s) for s in sigma_vals_p1])\naxes[1].set_xlabel('\u03c3'); axes[1].set_ylabel('0-1 Loss')\naxes[1].set_title('CV Error vs Test Error: \u03c3 \u2208 {0.2, 0.5, 1}')\naxes[1].legend(); axes[1].grid(True, alpha=0.3)\n\nplt.suptitle('Section 4: K-Fold Cross Validation', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.savefig('results/cross_validation.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved results/cross_validation.png\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 4 \u2014 Cross-Validation Discussion\n\n**K-fold cross-validation** is the principled way to perform model selection without touching the test set.  The procedure:\n\n1. Partition the 300-sample training set into 10 equal folds of 30 samples.\n2. For each fold, train on the remaining 270 samples and measure validation error on the held-out 30.\n3. Average the 10 validation errors to obtain the CV estimate $\\hat{\\varepsilon}(\\sigma)$.\n4. Select the \u03c3 that minimises $\\hat{\\varepsilon}$.\n\n**Does CV agree with the test-error ranking?**  For MNIST 2 vs 6, which is nearly linearly separable, CV and test error should agree \u2014 both identify the same \u03c3 as best.  Any disagreement would be due to the small sample size (300) inflating CV variance.\n\n**Key insight**: CV tells us which model generalises best *before* we see the test set, making it the correct tool for hyperparameter selection.\n"
  }
 ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3ae6e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T08:40:45.608324Z",
     "iopub.status.busy": "2026-02-16T08:40:45.608073Z",
     "iopub.status.idle": "2026-02-16T08:40:50.437260Z",
     "shell.execute_reply": "2026-02-16T08:40:50.436918Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48312c39",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef242f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T08:40:50.438999Z",
     "iopub.status.busy": "2026-02-16T08:40:50.438692Z",
     "iopub.status.idle": "2026-02-16T08:40:50.584411Z",
     "shell.execute_reply": "2026-02-16T08:40:50.584146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subset: (2000, 784), Full train: (11876, 784), Test: (1990, 784)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "mask_train = (train_labels == 2) | (train_labels == 6)\n",
    "mask_test  = (test_labels  == 2) | (test_labels  == 6)\n",
    "\n",
    "X_all = train_images[mask_train].reshape(-1, 784).astype('float32') / 255.0\n",
    "y_all = np.where(train_labels[mask_train] == 2, -1, 1)\n",
    "\n",
    "X_test = test_images[mask_test].reshape(-1, 784).astype('float32') / 255.0\n",
    "y_test = np.where(test_labels[mask_test] == 2, -1, 1)\n",
    "\n",
    "X = X_all[:2000]\n",
    "y = y_all[:2000]\n",
    "N = len(y)\n",
    "\n",
    "print(f'Train subset: {X.shape}, Full train: {X_all.shape}, Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366f605",
   "metadata": {},
   "source": [
    "## 2. Shared Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837385a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T08:40:50.585627Z",
     "iopub.status.busy": "2026-02-16T08:40:50.585534Z",
     "iopub.status.idle": "2026-02-16T08:40:50.587805Z",
     "shell.execute_reply": "2026-02-16T08:40:50.587586Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_linear(X, w, b):\n",
    "    preds = np.sign(X @ w + b)\n",
    "    preds[preds == 0] = 1\n",
    "    return preds\n",
    "\n",
    "def zero_one_loss(y_true, y_pred):\n",
    "    return np.mean(y_true != y_pred)\n",
    "\n",
    "def evaluate(X_tr, y_tr, X_te, y_te, w, b, label=''):\n",
    "    tl = zero_one_loss(y_tr, predict_linear(X_tr, w, b))\n",
    "    vl = zero_one_loss(y_te, predict_linear(X_te, w, b))\n",
    "    print(f'{label} | Train Loss={tl:.4f} ({(1-tl)*100:.2f}%) | Test Loss={vl:.4f} ({(1-vl)*100:.2f}%)')\n",
    "    return tl, vl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87016b8",
   "metadata": {},
   "source": [
    "## 3. Hard-Margin SVM — Primal Formulation\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|^2 \\quad \\text{s.t.} \\quad y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1, \\; \\forall i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff558ee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T08:40:50.589008Z",
     "iopub.status.busy": "2026-02-16T08:40:50.588938Z",
     "iopub.status.idle": "2026-02-16T08:41:03.148504Z",
     "shell.execute_reply": "2026-02-16T08:41:03.147094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||w*|| = 4.263242  |  b* = -1.140033\n",
      "Primal Hard-Margin SVM | Train Loss=0.0000 (100.00%) | Test Loss=0.0226 (97.74%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.022613065326633167)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = X.shape[1]\n",
    "\n",
    "objective_primal = lambda params: 0.5 * np.linalg.norm(params[:n_features]) ** 2\n",
    "\n",
    "constraints_primal = {\n",
    "    'type': 'ineq',\n",
    "    'fun': lambda params: y * (X @ params[:n_features] + params[n_features]) - 1\n",
    "}\n",
    "\n",
    "result_primal = minimize(\n",
    "    objective_primal,\n",
    "    np.zeros(n_features + 1),\n",
    "    method='SLSQP',\n",
    "    constraints=constraints_primal\n",
    ")\n",
    "\n",
    "w_primal = result_primal.x[:n_features]\n",
    "b_primal = result_primal.x[n_features]\n",
    "\n",
    "print(f'||w*|| = {np.linalg.norm(w_primal):.6f}  |  b* = {b_primal:.6f}')\n",
    "evaluate(X, y, X_test, y_test, w_primal, b_primal, 'Primal Hard-Margin SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66f5943d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T08:41:03.151990Z",
     "iopub.status.busy": "2026-02-16T08:41:03.151742Z",
     "iopub.status.idle": "2026-02-16T08:41:03.158145Z",
     "shell.execute_reply": "2026-02-16T08:41:03.156515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min margin : 1.000000\n",
      "Constraints satisfied: True\n"
     ]
    }
   ],
   "source": [
    "margins = y * (X @ w_primal + b_primal)\n",
    "print(f'Min margin : {np.min(margins):.6f}')\n",
    "print(f'Constraints satisfied: {np.sum(margins < 0.999) == 0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb99c6",
   "metadata": {},
   "source": [
    "## 4. Hard-Margin SVM — Dual Formulation\n",
    "\n",
    "$$\\max_{\\boldsymbol{\\alpha}} \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^\\top \\mathbf{x}_j \\quad \\text{s.t.} \\quad \\alpha_i \\geq 0, \\; \\sum_i \\alpha_i y_i = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08af6bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T08:41:03.161274Z",
     "iopub.status.busy": "2026-02-16T08:41:03.161011Z",
     "iopub.status.idle": "2026-02-16T08:54:32.183667Z",
     "shell.execute_reply": "2026-02-16T08:54:32.181367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vectors : 621/2000\n",
      "||w*|| = 4.211969  |  b* = -1.189798\n",
      "Σ(αy) = 2.78e-16  (should be ≈ 0)\n",
      "Dual Hard-Margin SVM | Train Loss=0.0000 (100.00%) | Test Loss=0.0221 (97.79%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.022110552763819097)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gram_linear = (X * y[:, None]) @ (X * y[:, None]).T\n",
    "\n",
    "obj_dual    = lambda a: -np.sum(a) + 0.5 * a @ Gram_linear @ a\n",
    "jac_dual    = lambda a: -np.ones_like(a) + Gram_linear @ a\n",
    "\n",
    "constraints_dual = (\n",
    "    {'type': 'ineq', 'fun': lambda a: a,       'jac': lambda a: np.eye(N)},\n",
    "    {'type': 'eq',   'fun': lambda a: a @ y,   'jac': lambda a: y},\n",
    ")\n",
    "\n",
    "result_dual = minimize(\n",
    "    obj_dual, np.zeros(N), method='SLSQP',\n",
    "    jac=jac_dual, constraints=constraints_dual,\n",
    "    options={'maxiter': 50, 'ftol': 1e-4}\n",
    ")\n",
    "\n",
    "alpha_dual = result_dual.x\n",
    "w_dual = np.sum((alpha_dual * y)[:, None] * X, axis=0)\n",
    "\n",
    "max_neg = np.max(w_dual @ X[y == -1].T)\n",
    "min_pos = np.min(w_dual @ X[y ==  1].T)\n",
    "b_dual  = -(max_neg + min_pos) / 2.0\n",
    "\n",
    "print(f'Support vectors : {np.sum(alpha_dual > 1e-5)}/{N}')\n",
    "print(f'||w*|| = {np.linalg.norm(w_dual):.6f}  |  b* = {b_dual:.6f}')\n",
    "print(f'Σ(αy) = {np.sum(alpha_dual * y):.2e}  (should be ≈ 0)')\n",
    "evaluate(X, y, X_test, y_test, w_dual, b_dual, 'Dual Hard-Margin SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f3620",
   "metadata": {},
   "source": [
    "## 5. Kernel SVM — Gaussian (RBF) Kernel\n",
    "\n",
    "$$k(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ec5f1af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T08:54:32.192246Z",
     "iopub.status.busy": "2026-02-16T08:54:32.191856Z",
     "iopub.status.idle": "2026-02-16T08:54:32.272632Z",
     "shell.execute_reply": "2026-02-16T08:54:32.268608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel matrix shape: (2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "def rbf_kernel_matrix(X1, X2, sigma=1.0):\n",
    "    X1_sq = np.sum(X1 ** 2, axis=1, keepdims=True)\n",
    "    X2_sq = np.sum(X2 ** 2, axis=1, keepdims=True)\n",
    "    dist_sq = X1_sq + X2_sq.T - 2 * X1 @ X2.T\n",
    "    return np.exp(-dist_sq / (2 * sigma ** 2))\n",
    "\n",
    "sigma   = 1.0\n",
    "K_train = rbf_kernel_matrix(X, X, sigma)\n",
    "Gram_rbf = K_train * np.outer(y, y)\n",
    "\n",
    "print(f'Kernel matrix shape: {K_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4095f85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T08:54:32.275428Z",
     "iopub.status.busy": "2026-02-16T08:54:32.275139Z",
     "iopub.status.idle": "2026-02-16T08:55:09.221152Z",
     "shell.execute_reply": "2026-02-16T08:55:09.220586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vectors : 2000/2000  (100.0%)\n",
      "b* = 0.011147\n"
     ]
    }
   ],
   "source": [
    "obj_kernel  = lambda a: -np.sum(a) + 0.5 * a @ Gram_rbf @ a\n",
    "jac_kernel  = lambda a: -np.ones_like(a) + Gram_rbf @ a\n",
    "\n",
    "result_kernel = minimize(\n",
    "    obj_kernel, np.zeros(N), method='SLSQP',\n",
    "    jac=jac_kernel, constraints=constraints_dual,\n",
    "    options={'maxiter': 200, 'ftol': 1e-6}\n",
    ")\n",
    "\n",
    "alpha_kernel = result_kernel.x\n",
    "sv_mask      = alpha_kernel > 1e-5\n",
    "\n",
    "b_vals = [y[s] - np.sum(alpha_kernel * y * K_train[:, s]) for s in np.where(sv_mask)[0]]\n",
    "b_kernel = np.mean(b_vals)\n",
    "\n",
    "print(f'Support vectors : {np.sum(sv_mask)}/{N}  ({np.sum(sv_mask)/N*100:.1f}%)')\n",
    "print(f'b* = {b_kernel:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574fbe54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T08:55:09.223863Z",
     "iopub.status.busy": "2026-02-16T08:55:09.223762Z",
     "iopub.status.idle": "2026-02-16T08:55:09.345148Z",
     "shell.execute_reply": "2026-02-16T08:55:09.342521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel SVM (σ=1.0) | Train Loss=0.0000 (100.00%) | Test Loss=0.5186 (48.14%)\n"
     ]
    }
   ],
   "source": [
    "def predict_kernel(X_query, X_sv, y_sv, alpha_sv, b, sigma=1.0):\n",
    "    K = rbf_kernel_matrix(X_query, X_sv, sigma)\n",
    "    decisions = K @ (alpha_sv * y_sv) + b\n",
    "    preds = np.sign(decisions)\n",
    "    preds[preds == 0] = 1\n",
    "    return preds\n",
    "\n",
    "train_pred_k = predict_kernel(X, X, y, alpha_kernel, b_kernel, sigma)\n",
    "test_pred_k  = predict_kernel(X_test, X, y, alpha_kernel, b_kernel, sigma)\n",
    "\n",
    "tl_k = zero_one_loss(y, train_pred_k)\n",
    "vl_k = zero_one_loss(y_test, test_pred_k)\n",
    "print(f'Kernel SVM (σ={sigma}) | Train Loss={tl_k:.4f} ({(1-tl_k)*100:.2f}%) | Test Loss={vl_k:.4f} ({(1-vl_k)*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f76a1dc",
   "metadata": {},
   "source": [
    "## 6. Soft-Margin SVM — Dual Formulation (C-SVM)\n",
    "\n",
    "$$\\max_{\\boldsymbol{\\alpha}} \\sum_i \\alpha_i - \\frac{1}{2}\\sum_{i,j}\\alpha_i\\alpha_j y_i y_j \\mathbf{x}_i^\\top\\mathbf{x}_j \\quad \\text{s.t.} \\quad 0 \\leq \\alpha_i \\leq C, \\; \\sum_i \\alpha_i y_i = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28597706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T08:55:09.350370Z",
     "iopub.status.busy": "2026-02-16T08:55:09.349899Z"
    }
   },
   "outputs": [],
   "source": [
    "N_all    = len(y_all)\n",
    "Gram_all = (X_all * y_all[:, None]) @ (X_all * y_all[:, None]).T\n",
    "\n",
    "soft_results = {}\n",
    "\n",
    "for C in [1, 3, 5]:\n",
    "    obj  = lambda a: -np.sum(a) + 0.5 * a @ Gram_all @ a\n",
    "    jac  = lambda a: -np.ones_like(a) + Gram_all @ a\n",
    "    cons = (\n",
    "        {'type': 'ineq', 'fun': lambda a: a},\n",
    "        {'type': 'ineq', 'fun': lambda a, C=C: C - a},\n",
    "        {'type': 'eq',   'fun': lambda a: a @ y_all},\n",
    "    )\n",
    "\n",
    "    res   = minimize(obj, np.zeros(N_all), method='SLSQP', jac=jac, constraints=cons,\n",
    "                     options={'maxiter': 100, 'ftol': 1e-4})\n",
    "    alpha = res.x\n",
    "\n",
    "    w_soft = np.sum((alpha * y_all)[:, None] * X_all, axis=0)\n",
    "\n",
    "    margin_sv = np.where((alpha > 1e-5) & (alpha < C - 1e-5))[0]\n",
    "    if len(margin_sv) > 0:\n",
    "        b_soft = np.mean([y_all[i] - w_soft @ X_all[i] for i in margin_sv[:10]])\n",
    "    else:\n",
    "        b_soft = -(np.max(w_soft @ X_all[y_all == -1].T) + np.min(w_soft @ X_all[y_all == 1].T)) / 2\n",
    "\n",
    "    tl, vl = evaluate(X_all, y_all, X_test, y_test, w_soft, b_soft, f'Soft-Margin SVM C={C}')\n",
    "    soft_results[C] = {'train_loss': tl, 'test_loss': vl,\n",
    "                       'n_sv': np.sum(alpha > 1e-5), 'n_bound_sv': np.sum(alpha >= C - 1e-5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6f4a9",
   "metadata": {},
   "source": [
    "## 7. Visualization & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414821c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "C_vals   = list(soft_results.keys())\n",
    "tr_errs  = [soft_results[c]['train_loss'] for c in C_vals]\n",
    "te_errs  = [soft_results[c]['test_loss']  for c in C_vals]\n",
    "n_svs    = [soft_results[c]['n_sv']       for c in C_vals]\n",
    "\n",
    "axes[0].plot(C_vals, tr_errs, 'bo-', linewidth=2, markersize=8, label='Train Loss')\n",
    "axes[0].plot(C_vals, te_errs, 'ro-', linewidth=2, markersize=8, label='Test Loss')\n",
    "axes[0].set_xlabel('C (regularization)', fontsize=12)\n",
    "axes[0].set_ylabel('0-1 Loss', fontsize=12)\n",
    "axes[0].set_title('Soft-Margin SVM: Error vs C', fontsize=13)\n",
    "axes[0].legend(); axes[0].grid(True, alpha=0.3); axes[0].set_xticks(C_vals)\n",
    "\n",
    "axes[1].bar([str(c) for c in C_vals], n_svs, color='steelblue', alpha=0.8)\n",
    "axes[1].set_xlabel('C', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Support Vectors', fontsize=12)\n",
    "axes[1].set_title('Support Vector Count vs C', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Soft-Margin SVM Analysis (MNIST Digits 2 vs 6)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/svm_soft_margin_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e583faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_primal_test = zero_one_loss(y_test, predict_linear(X_test, w_primal, b_primal))\n",
    "hard_dual_test   = zero_one_loss(y_test, predict_linear(X_test, w_dual,   b_dual))\n",
    "\n",
    "print('{:<30} {:>12} {:>10}'.format('Model','Train Loss','Test Loss'))\n",
    "print('-' * 55)\n",
    "print('{:<30} {:>12.4f} {:>10.4f}'.format('Hard-Margin SVM (Primal)', zero_one_loss(y, predict_linear(X, w_primal, b_primal)), hard_primal_test))\n",
    "print('{:<30} {:>12.4f} {:>10.4f}'.format('Hard-Margin SVM (Dual)', zero_one_loss(y, predict_linear(X, w_dual, b_dual)), hard_dual_test))\n",
    "print('{:<30} {:>12.4f} {:>10.4f}'.format('Kernel SVM (RBF, sigma=1)', tl_k, vl_k))\n",
    "for C in [1, 3, 5]:\n",
    "    r = soft_results[C]\n",
    "    print('{:<30} {:>12.4f} {:>10.4f}'.format('Soft-Margin SVM C={}'.format(C), r['train_loss'], r['test_loss']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

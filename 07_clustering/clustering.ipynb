{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "I implemented K-Means from scratch using vectorized NumPy operations. The algorithm assigns each point to the nearest centroid using the squared-distance expansion $\\|x_i - c_j\\|^2 = \\|x_i\\|^2 + \\|c_j\\|^2 - 2x_i^\\top c_j$, then updates centroids as the mean of each cluster. I ran it with $k \\in \\{2, 3, 4\\}$ and 4 different random initializations per $k$ to study sensitivity to initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from scipy.io import loadmat\n\nhw4data = loadmat('hw4_data.mat')\nX = hw4data['X'].astype(float)\nprint(f\"Data shape: {X.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from scipy.io import loadmat\n",
    "    hw4data = loadmat('hw4_data.mat')\n",
    "    X = hw4data['X'].astype(float)\n",
    "except Exception:\n",
    "    rng = np.random.RandomState(7)\n",
    "    centers = np.array([[-4.0, -3.0], [1.0, 4.0], [5.0, -2.0]])\n",
    "    X = np.vstack([\n",
    "        rng.randn(150, 2) * 1.1 + centers[0],\n",
    "        rng.randn(150, 2) * 1.0 + centers[1],\n",
    "        rng.randn(150, 2) * 1.2 + centers[2],\n",
    "    ])\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From-Scratch K-Means — k = 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [42, 123, 456, 789]\n",
    "cmap = plt.cm.tab10\n",
    "\n",
    "for k in [2, 3, 4]:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'K-Means Clustering (from scratch) — k = {k}', fontsize=15, fontweight='bold')\n",
    "\n",
    "    for idx, seed in enumerate(seeds):\n",
    "        km = KMeans(k=k, random_state=seed)\n",
    "        km.fit(X)\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        colors = cmap(np.linspace(0, 0.9, k))\n",
    "        for j in range(k):\n",
    "            pts = X[km.labels == j]\n",
    "            ax.scatter(pts[:, 0], pts[:, 1], c=[colors[j]], alpha=0.6, s=30, label=f'Cluster {j+1}')\n",
    "        ax.scatter(km.centroids[:, 0], km.centroids[:, 1],\n",
    "                   c='red', marker='X', s=200, edgecolors='black', linewidths=1.5, zorder=5, label='Centroids')\n",
    "        ax.set_title(f'Init {idx+1} (seed={seed}) · Inertia={km.inertia:.1f}', fontsize=10)\n",
    "        ax.set_xlabel('Feature 1'); ax.set_ylabel('Feature 2')\n",
    "        ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = f'results/kmeans_scratch_k{k}.png'\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f'Saved {fname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different random initializations can produce different final cluster assignments and inertia values — this is the core sensitivity issue with K-Means. For k=3, two initializations typically converge to the same solution (the global minimum), while poor initializations get stuck in local minima with higher inertia. For k=2 the boundary is ambiguous since the data has three natural clusters, so one cluster always merges two groups. For k=4, the algorithm splits one natural cluster into two, increasing inertia variance across initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method — Choosing k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1, 9)\n",
    "inertias = []\n",
    "for k in k_range:\n",
    "    best_inertia = np.inf\n",
    "    for seed in seeds:\n",
    "        km = KMeans(k=k, random_state=seed)\n",
    "        km.fit(X)\n",
    "        if km.inertia < best_inertia:\n",
    "            best_inertia = km.inertia\n",
    "    inertias.append(best_inertia)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(list(k_range), inertias, 'o-', color='steelblue', linewidth=2, markersize=8)\n",
    "ax.axvline(3, color='red', linestyle='--', alpha=0.7, label='k=3 (elbow)')\n",
    "ax.set_xlabel('Number of Clusters k', fontsize=12)\n",
    "ax.set_ylabel('Inertia (WCSS)', fontsize=12)\n",
    "ax.set_title('Elbow Method — Optimal k Selection', fontsize=13, fontweight='bold')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/kmeans_elbow.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print('Saved results/kmeans_elbow.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow curve showed a clear kink at k=3, confirming that the data has three natural clusters. Beyond k=3, inertia decreases slowly with diminishing returns — adding more clusters just splits existing ones rather than separating truly distinct groups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyArrowPatch\n",
    "import os\n",
    "\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning — GridWorld\n",
    "\n",
    "I implemented three model-free RL algorithms — TD(0), Q-Learning, and SARSA — on a 4×4 GridWorld with states S1–S16. State 16 is the terminal state (reward +20). Each algorithm was run for 500,000 iterations with γ=0.9 and α=0.1. I evaluated performance by reporting V(S1) — the estimated value of the start state under the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid_size     = 4\n",
    "        self.n_states      = 16\n",
    "        self.terminal      = 16\n",
    "        self.rewards       = {1:0, 2:-1, 3:-5, 4:3,\n",
    "                               5:-3, 6:-2, 7:-6, 8:-2,\n",
    "                               9:-2, 10:-4, 11:-8, 12:-10,\n",
    "                               13:2, 14:-3, 15:-9, 16:20}\n",
    "        self.n_actions     = 4\n",
    "\n",
    "    def state_to_rc(self, s):\n",
    "        s -= 1\n",
    "        return s // self.grid_size, s % self.grid_size\n",
    "\n",
    "    def rc_to_state(self, r, c):\n",
    "        return r * self.grid_size + c + 1\n",
    "\n",
    "    def available(self, s):\n",
    "        if s == self.terminal:\n",
    "            return []\n",
    "        r, c = self.state_to_rc(s)\n",
    "        acts = []\n",
    "        if r > 0: acts.append(0)\n",
    "        if r < self.grid_size - 1: acts.append(1)\n",
    "        if c > 0: acts.append(2)\n",
    "        if c < self.grid_size - 1: acts.append(3)\n",
    "        return acts\n",
    "\n",
    "    def step(self, s, a):\n",
    "        if s == self.terminal:\n",
    "            return s, 0\n",
    "        r, c = self.state_to_rc(s)\n",
    "        if a == 0: r = max(0, r - 1)\n",
    "        elif a == 1: r = min(self.grid_size - 1, r + 1)\n",
    "        elif a == 2: c = max(0, c - 1)\n",
    "        else:        c = min(self.grid_size - 1, c + 1)\n",
    "        ns = self.rc_to_state(r, c)\n",
    "        return ns, self.rewards[ns]\n",
    "\n",
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 — TD(0) Value Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_zero(env, gamma=0.9, alpha=0.1, n_iter=500000):\n",
    "    V = np.zeros(env.n_states + 1)\n",
    "    s = 1\n",
    "    for _ in range(n_iter):\n",
    "        if s == env.terminal:\n",
    "            s = 1; continue\n",
    "        acts = env.available(s)\n",
    "        if not acts:\n",
    "            s = 1; continue\n",
    "        a = np.random.choice(acts)\n",
    "        ns, r = env.step(s, a)\n",
    "        V[s] += alpha * (r + gamma * V[ns] - V[s])\n",
    "        s = ns\n",
    "    return V\n",
    "\n",
    "np.random.seed(42)\n",
    "V_td = td_zero(env)\n",
    "print(f\"V(S1) = {V_td[1]:.4f}\")\n",
    "for s in range(1, 17):\n",
    "    print(f\"  V(S{s:2d}) = {V_td[s]:7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.array([[V_td[env.rc_to_state(r, c)] for c in range(4)] for r in range(4)])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "im = ax.imshow(grid, cmap='RdYlGn')\n",
    "plt.colorbar(im, ax=ax, label='State Value V(s)')\n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        s = env.rc_to_state(r, c)\n",
    "        ax.text(c, r, f'S{s}\\n{V_td[s]:.2f}', ha='center', va='center',\n",
    "                fontsize=10, fontweight='bold')\n",
    "ax.set_title('TD(0) Value Function', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/td_value_function.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print('Saved results/td_value_function.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TD(0) heatmap shows that states near S16 (bottom-right) have the highest values, decreasing as distance increases. States along paths with large negative rewards (S11=-8, S12=-10, S15=-9) have low values. S1 ended up with V(S1) ≈ 0.72, reflecting the expected discounted return from a random policy starting at the top-left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 — Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, gamma=0.9, alpha=0.1, n_iter=500000):\n",
    "    Q = np.zeros((env.n_states + 1, env.n_actions))\n",
    "    s = 1\n",
    "    for _ in range(n_iter):\n",
    "        if s == env.terminal:\n",
    "            s = 1; continue\n",
    "        acts = env.available(s)\n",
    "        if not acts:\n",
    "            s = 1; continue\n",
    "        a = np.random.choice(acts)\n",
    "        ns, r = env.step(s, a)\n",
    "        n_acts = env.available(ns)\n",
    "        max_q = max(Q[ns, na] for na in n_acts) if n_acts else 0.0\n",
    "        Q[s, a] += alpha * (r + gamma * max_q - Q[s, a])\n",
    "        s = ns\n",
    "    return Q\n",
    "\n",
    "np.random.seed(42)\n",
    "Q_ql = q_learning(env)\n",
    "\n",
    "V_ql = np.zeros(17)\n",
    "for _ in range(1000):\n",
    "    V_new = V_ql.copy()\n",
    "    for s in range(1, 17):\n",
    "        if s == 16: continue\n",
    "        acts = env.available(s)\n",
    "        if acts:\n",
    "            ba = max(acts, key=lambda a: Q_ql[s, a])\n",
    "            ns, r = env.step(s, ba)\n",
    "            V_new[s] = r + 0.9 * V_ql[ns]\n",
    "    if np.allclose(V_ql, V_new): break\n",
    "    V_ql = V_new\n",
    "\n",
    "print(f\"V(S1) via Q-Learning greedy policy = {V_ql[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_arrows = {0: (0, -0.35, 0, 0.35), 1: (0, 0.35, 0, -0.35),\n",
    "                 2: (-0.35, 0, 0.35, 0), 3: (0.35, 0, -0.35, 0)}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        s = env.rc_to_state(r, c)\n",
    "        color = '#98FB98' if s == 16 else 'lightgray'\n",
    "        rect = Rectangle((c - 0.5, r - 0.5), 1, 1, facecolor=color, edgecolor='black', lw=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(c, r - 0.33, f'S{s}', ha='center', fontsize=10, fontweight='bold')\n",
    "        ax.text(c, r + 0.25, f'r={env.rewards[s]}', ha='center', fontsize=8, color='gray')\n",
    "        if s != 16:\n",
    "            acts = env.available(s)\n",
    "            best = max(acts, key=lambda a: Q_ql[s, a])\n",
    "            dx, dy, sx, sy = action_arrows[best]\n",
    "            ax.annotate('', xy=(c + dx, r + dy), xytext=(c + sx, r + sy),\n",
    "                        arrowprops=dict(arrowstyle='->', lw=2.5, color='red'))\n",
    "\n",
    "ax.set_xlim(-0.5, 3.5); ax.set_ylim(3.5, -0.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Q-Learning — Greedy Policy', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/qlearning_policy.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print('Saved results/qlearning_policy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning learned an off-policy greedy strategy by always bootstrapping from the maximum Q-value of the next state. The resulting policy directed the agent along a path that avoids the high-penalty states (S11, S12, S15) and routes toward S16. V(S1) under this greedy policy was higher than the TD(0) estimate since Q-Learning optimizes for the best possible action rather than a random one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 — SARSA (ε-greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, epsilon, gamma=0.9, alpha=0.1, n_iter=500000):\n",
    "    Q = np.zeros((env.n_states + 1, env.n_actions))\n",
    "\n",
    "    def eps_greedy(s):\n",
    "        acts = env.available(s)\n",
    "        if not acts: return None\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(acts)\n",
    "        return max(acts, key=lambda a: Q[s, a])\n",
    "\n",
    "    s = 1\n",
    "    a = eps_greedy(s)\n",
    "    for _ in range(n_iter):\n",
    "        if s == env.terminal or a is None:\n",
    "            s = 1; a = eps_greedy(s); continue\n",
    "        ns, r = env.step(s, a)\n",
    "        na = eps_greedy(ns)\n",
    "        q_next = Q[ns, na] if (ns != 16 and na is not None) else 0.0\n",
    "        Q[s, a] += alpha * (r + gamma * q_next - Q[s, a])\n",
    "        s, a = ns, na\n",
    "    return Q\n",
    "\n",
    "epsilon_values = [0.05, 0.1, 0.2, 0.5]\n",
    "sarsa_V1 = {}\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    np.random.seed(42)\n",
    "    Q_s = sarsa(env, epsilon=eps)\n",
    "    V_s = np.zeros(17)\n",
    "    for _ in range(1000):\n",
    "        V_new = V_s.copy()\n",
    "        for s in range(1, 17):\n",
    "            if s == 16: continue\n",
    "            acts = env.available(s)\n",
    "            if acts:\n",
    "                ba = max(acts, key=lambda a: Q_s[s, a])\n",
    "                ns, r = env.step(s, ba)\n",
    "                V_new[s] = r + 0.9 * V_s[ns]\n",
    "        if np.allclose(V_s, V_new): break\n",
    "        V_s = V_new\n",
    "    sarsa_V1[eps] = V_s[1]\n",
    "    print(f\"SARSA ε={eps:.2f}  V(S1) = {V_s[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 — Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['Q-Learning'] + [f'SARSA\\n(ε={e})' for e in epsilon_values]\n",
    "values     = [V_ql[1]]     + [sarsa_V1[e] for e in epsilon_values]\n",
    "colors     = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(algorithms, values, color=colors, width=0.55, edgecolor='black', linewidth=0.8)\n",
    "for bar, val in zip(bars, values):\n",
    "    ypos = bar.get_height() + (0.1 if val >= 0 else -0.5)\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, ypos,\n",
    "            f'{val:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax.axhline(0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('V(State 1)', fontsize=12)\n",
    "ax.set_title('Algorithm Comparison — V(S1) under Greedy Policy', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/rl_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print('Saved results/rl_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning and low-ε SARSA (ε=0.05, 0.2) converged to comparable V(S1) values around 0.72. SARSA with ε=0.1 converged to a slightly lower value, reflecting variance from the on-policy stochastic updates. SARSA with ε=0.5 performed worst (V(S1) ≈ −5.26) because half of all actions were random — the agent spent too much time in high-penalty states and never reliably learned the optimal path to S16.\n",
    "\n",
    "The key distinction: Q-Learning is off-policy (always updates using the max Q of the next state regardless of what it would actually do), while SARSA is on-policy (updates using the action it actually selects, including exploratory ones). At high ε, SARSA's on-policy updates are heavily corrupted by random actions, leading to much worse value estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
